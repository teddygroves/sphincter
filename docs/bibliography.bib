@article{gelmanBayesianWorkflow2020,
	title = {Bayesian Workflow},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian
	          , Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren
	          and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k,
	          Martin},
	year = {2020},
	month = {nov},
	abstract = {The Bayesian approach to data analysis provides a powerful way
	            to handle uncertainty in all observations, model parameters, and
	            model structure using probability theory. Probabilistic
	            programming languages make it easier to specify and fit Bayesian
	            models, but this still leaves us with many options regarding
	            constructing, evaluating, and using these models, along with many
	            remaining challenges in computation. Using Bayesian inference to
	            solve real-world problems requires not only statistical skills,
	            subject matter knowledge, and programming, but also awareness of
	            the decisions made in the process of data analysis. All of these
	            aspects can be understood as part of a tangled workflow of
	            applied Bayesian statistics. Beyond inference, the workflow also
	            includes iterative model building, model checking, validation and
	            troubleshooting of computational problems, model understanding,
	            and model comparison. We review all these aspects of workflow in
	            the context of several examples, keeping in mind that in practice
	            we will be fitting many models for any given problem, even if
	            only a subset of them will ultimately be relevant for our
	            conclusions.},
	archiveprefix = {arXiv},
	eprint = {2011.01808},
	eprinttype = {arxiv},
	file = {/Users/tedgro/Zotero/storage/XEI2Q7F4/Gelman et al. - 2020 -
	        Bayesian Workflow.pdf;/Users/tedgro/Zotero/storage/8Y5YAYXE/2011.html
	        },
	journal = {arXiv:2011.01808 [stat]},
	keywords = {Statistics - Methodology},
	primaryclass = {stat},
}

@article{spiegelhalterBayesianGraphicalModelling1998,
	title = {Bayesian {{Graphical Modelling}}: {{A Case-Study}} in {{Monitoring
	         Health Outcomes}}},
	shorttitle = {Bayesian {{Graphical Modelling}}},
	author = {Spiegelhalter, David J.},
	year = {1998},
	month = mar,
	journal = {Journal of the Royal Statistical Society Series C: Applied
	           Statistics},
	volume = {47},
	number = {1},
	pages = {115--133},
	issn = {0035-9254, 1467-9876},
	doi = {10.1111/1467-9876.00101},
	url = {https://academic.oup.com/jrsssc/article/47/1/115/6990621},
	urldate = {2023-11-03},
	abstract = {Bayesian graphical modelling represents the synthesis of several
	            recent developments in applied complex modelling. After
	            describing a moderately challenging real example, we show how
	            graphical models and Markov chain Monte Carlo methods naturally
	            provide a direct path between model speci\textregistered cation
	            and the computational means of making inferences on that model.
	            These ideas are illustrated with a range of modelling issues
	            related to our example. An appendix discusses the BUGS software.},
	langid = {english},
	file = {/Users/tedgro/Zotero/storage/IP9KAM85/Spiegelhalter - 1998 -
	        Bayesian Graphical Modelling A Case-Study in Moni.pdf},
}

@article{tornqvistHowShouldRelative1985,
	title = {How {{Should Relative Changes Be Measured}}?},
	author = {Tornqvist, Leo and Vartia, Pentti and Vartia, Yrjo O.},
	year = {1985},
	journal = {The American Statistician},
	volume = {39},
	number = {1},
	eprint = {2683905},
	eprinttype = {jstor},
	pages = {43--46},
	publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
	issn = {0003-1305},
	doi = {10.2307/2683905},
	url = {https://www.jstor.org/stable/2683905},
	urldate = {2023-11-07},
	abstract = {Various indicators of relative change (or difference) are
	            considered. It is shown that the log change defined by loge (v/x)
	            is the only symmetric, additive, and normed indicator of relative
	            change. It is proposed that the values of the log change in
	            percent, 100 loge(y/x), be denoted by the symbol L\%, the log
	            percentage. It is hoped that the symmetric and additive log
	            percentages will gradually replace the ordinary asymmetric and
	            nonadditive percentages.},
	file = {/Users/tedgro/Zotero/storage/2ZITCFPK/Tornqvist et al. - 1985 - How
	        Should Relative Changes Be Measured.pdf},
}

@article{juarezModelBasedClusteringNonGaussian2010,
	title = {Model-{{Based Clustering}} of {{Non-Gaussian Panel Data Based}} on
	         {{Skew-t Distributions}}},
	author = {Ju{\'a}rez, Miguel A. and Steel, Mark F. J.},
	year = {2010},
	month = jan,
	journal = {Journal of Business \& Economic Statistics},
	volume = {28},
	number = {1},
	pages = {52--66},
	publisher = {{Taylor \& Francis}},
	issn = {0735-0015},
	doi = {10.1198/jbes.2009.07145},
	url = {https://doi.org/10.1198/jbes.2009.07145},
	urldate = {2021-09-07},
	abstract = {We propose a model-based method to cluster units within a panel.
	            The underlying model is autoregressive and non-Gaussian, allowing
	            for both skewness and fat tails, and the units are clustered
	            according to their dynamic behavior, equilibrium level, and the
	            effect of covariates. Inference is addressed from a Bayesian
	            perspective, and model comparison is conducted using Bayes
	            factors. Particular attention is paid to prior elicitation and
	            posterior propriety. We suggest priors that require little
	            subjective input and have hierarchical structures that enhance
	            inference robustness. We apply our methodology to GDP growth of
	            European regions and to employment growth of Spanish firms.},
	keywords = {Autoregressive modeling,Employment growth,GDP growth convergence
	            ,Hierarchical prior,Model comparison,Posterior propriety,Skewness
	            },
	file = {/Users/tedgro/Zotero/storage/EL3ZEJ39/Ju√°rez and Steel - 2010 -
	        Model-Based Clustering of Non-Gaussian Panel Data
	        .pdf;/Users/tedgro/Zotero/storage/ZFXS2WUD/jbes.2009.html},
}

@misc{standevelopmentteamCmdStanPy2022,
	title = {{{CmdStanPy}}},
	author = {{Stan Development Team}},
	year = {2022},
	url = {https://github.com/stan-dev/cmdstanpy},
}

@article{kumarArviZUnifiedLibrary2019,
	title = {{{ArviZ}} a Unified Library for Exploratory Analysis of {{Bayesian}
	         } Models in {{Python}}},
	author = {Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin,
	          Osvaldo},
	year = {2019},
	month = jan,
	journal = {Journal of Open Source Software},
	volume = {4},
	number = {33},
	pages = {1143},
	issn = {2475-9066},
	doi = {10.21105/joss.01143},
	url = {http://joss.theoj.org/papers/10.21105/joss.01143},
	urldate = {2021-12-02},
	abstract = {While conceptually simple, Bayesian methods can be
	            mathematically and numerically challenging. Probabilistic
	            programming languages (PPLs) implement functions to easily build
	            Bayesian models together with efficient automatic inference
	            methods. This helps separate the model building from the
	            inference, allowing practitioners to focus on their specific
	            problems and leaving PPLs to handle the computational details for
	            them (Bessiere, Mazer, Ahuactzin, \& Mekhnacha, 2013; Daniel Roy,
	            2015; Ghahramani, 2015). The inference process generates a
	            posterior distribution \textemdash{} which has a central role in
	            Bayesian statistics \textemdash{} together with other
	            distributions like the posterior predictive distribution and the
	            prior predictive distribution. The correct visualization,
	            analysis, and interpretation of these distributions is key to
	            properly answer the questions that motivate the inference
	            process.},
	langid = {english},
	file = {/Users/tedgro/Zotero/storage/HT4JS7NP/Kumar et al. - 2019 - ArviZ a
	        unified library for exploratory analysis o.pdf},
}

@software{bibat,
	doi = {10.5281/zenodo.7775328},
	url = {https://github.com/teddygroves/bibat},
	year = {2023},
	author = {Teddy Groves},
	title = {Bibat: batteries-included Bayesian analysis template},
}

@article{vehtariRankNormalizationFoldingLocalization2021,
	title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An
	         Improved R\textasciicircum}} for {{Assessing Convergence}} of {{MCMC
	         }} (with {{Discussion}})},
	shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
	author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter,
	          Bob and B{\"u}rkner, Paul-Christian},
	year = {2021},
	month = jun,
	journal = {Bayesian Analysis},
	volume = {16},
	number = {2},
	pages = {667--718},
	publisher = {{International Society for Bayesian Analysis}},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/20-BA1221},
	url = {
	       https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full
	       },
	urldate = {2022-01-03},
	abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian
	            statistics, but it can be challenging to monitor the convergence
	            of an iterative stochastic algorithm. In this paper we show that
	            the convergence diagnostic R\textasciicircum{} of Gelman and
	            Rubin (1992) has serious flaws. Traditional R\textasciicircum{}
	            will fail to correctly diagnose convergence failures when the
	            chain has a heavy tail or when the variance varies across the
	            chains. In this paper we propose an alternative rank-based
	            diagnostic that fixes these problems. We also introduce a
	            collection of quantile-based local efficiency measures, along
	            with a practical approach for computing Monte Carlo error
	            estimates for quantiles. We suggest that common trace plots
	            should be replaced with rank plots from multiple chains. Finally,
	            we give recommendations for how these methods should be used in
	            practice.},
	file = {/Users/tedgro/Zotero/storage/FPRAEFHZ/Vehtari et al. - 2021 -
	        Rank-Normalization, Folding, and Localization An
	        .pdf;/Users/tedgro/Zotero/storage/ST53UIWZ/20-BA1221.html},
}

@article{vehtariPracticalBayesianModel2017,
	title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out
	         Cross-Validation and {{WAIC}}},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	year = {2017},
	month = sep,
	journal = {Statistics and Computing},
	volume = {27},
	number = {5},
	eprint = {1507.04544},
	pages = {1413--1432},
	issn = {0960-3174, 1573-1375},
	doi = {10.1007/s11222-016-9696-4},
	url = {http://arxiv.org/abs/1507.04544},
	urldate = {2021-02-23},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable
	            information criterion (WAIC) are methods for estimating pointwise
	            out-of-sample prediction accuracy from a fitted Bayesian model
	            using the log-likelihood evaluated at the posterior simulations
	            of the parameter values. LOO and WAIC have various advantages
	            over simpler estimates of predictive error such as AIC and DIC
	            but are less used in practice because they involve additional
	            computational steps. Here we lay out fast and stable computations
	            for LOO and WAIC that can be performed using existing simulation
	            draws. We introduce an efficient computation of LOO using
	            Pareto-smoothed importance sampling (PSIS), a new procedure for
	            regularizing importance weights. Although WAIC is asymptotically
	            equal to LOO, we demonstrate that PSIS-LOO is more robust in the
	            finite case with weak priors or influential observations. As a
	            byproduct of our calculations, we also obtain approximate
	            standard errors for estimated predictive errors and for
	            comparison of predictive errors between two models. We implement
	            the computations in an R package called loo and demonstrate using
	            models fit with the Bayesian inference package Stan.},
	archiveprefix = {arxiv},
	langid = {english},
	keywords = {Statistics - Computation,Statistics - Methodology},
	file = {/Users/tedgro/Zotero/storage/554NNNS2/Vehtari et al. - 2017 -
	        Practical Bayesian model evaluation using leave-on.pdf},
}
@misc{betancourtDiagnosingBiasedInference2017,
	title = {Diagnosing {{Biased Inference}} with {{Divergences}}},
	author = {Betancourt, Michael},
	year = {2017},
	journal = {betanalpha.github.io},
	url = {
	       https://github.com/betanalpha/knitr_case_studies/tree/master/divergences_and_bias
	       },
	urldate = {2023-02-13},
	langid = {english},
	annotation = {commit b474ec1a5a79347f7c9634376c866fe3294d657a},
	file = {/Users/tedgro/Zotero/storage/ZYV3R7VK/writing.html},
}
@inproceedings{niels_bantilan-proc-scipy-2020,
	title = {Pandera: {{Statistical Data Validation}} of {{Pandas Dataframes}}},
	booktitle = {Proceedings of the 19th {{Python}} in {{Science Conference}}},
	author = {{Niels Bantilan}},
	editor = {Agarwal, Meghann and Calloway, Chris and Niederhut, Dillon and {
	          David Shupe}},
	year = {2020},
	pages = {116--124},
	doi = {10.25080/Majora-342d178e-010},
}
@misc{pydanticdevelopersPydantic2022,
	title = {Pydantic},
	author = {{Pydantic developers}},
	year = {2022},
	url = {https://pypi.org/project/pydantic/},
}

@article{piironenSparsityInformationRegularization2017,
	title = {Sparsity Information and Regularization in the Horseshoe and Other
	         Shrinkage Priors},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	month = jan,
	journal = {Electronic Journal of Statistics},
	volume = {11},
	number = {2},
	issn = {1935-7524},
	doi = {10.1214/17-EJS1337SI},
	url = {
	       https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full
	       },
	urldate = {2023-03-13},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative
	            for sparse Bayesian estimation, but has previously suffered from
	            two problems. First, there has been no systematic way of
	            specifying a prior for the global shrinkage hyperparameter based
	            on the prior information about the degree of sparsity in the
	            parameter vector. Second, the horseshoe prior has the undesired
	            property that there is no possibility of specifying separately
	            information about sparsity and the amount of regularization for
	            the largest coefficients, which can be problematic with weakly
	            identified parameters, such as the logistic regression
	            coefficients in the case of data separation. This paper proposes
	            solutions to both of these problems. We introduce a concept of
	            effective number of nonzero parameters, show an intuitive way of
	            formulating the prior for the global hyperparameter based on the
	            sparsity assumptions, and argue that the previous default choices
	            are dubious based on their tendency to favor solutions with more
	            unshrunk parameters than we typically expect a priori. Moreover,
	            we introduce a generalization to the horseshoe prior, called the
	            regularized horseshoe, that allows us to specify a minimum level
	            of regularization to the largest values. We show that the new
	            prior can be considered as the continuous counterpart of the
	            spike-and-slab prior with a finite slab width, whereas the
	            original horseshoe resembles the spike-and-slab with an
	            infinitely wide slab. Numerical experiments on synthetic and real
	            world data illustrate the benefit of both of these theoretical
	            advances.},
	langid = {english},
	file = {/Users/tedgro/Zotero/storage/ZUX4LE4Q/Piironen and Vehtari - 2017 -
	        Sparsity information and regularization in the hor.pdf},
}

@article{gaoImprovingMultilevelRegression2019,
	title = {Improving Multilevel Regression and Poststratification with
	         Structured Priors},
	author = {Gao, Yuxiang and Kennedy, Lauren and Simpson, Daniel and Gelman,
	          Andrew},
	year = {2019},
	month = sep,
	journal = {arXiv:1908.06716 [stat]},
	eprint = {1908.06716},
	primaryclass = {stat},
	url = {http://arxiv.org/abs/1908.06716},
	urldate = {2020-01-09},
	abstract = {A central theme in the field of survey statistics is estimating
	            population-level quantities through data coming from potentially
	            non-representative samples of the population. Multilevel
	            Regression and Poststratification (MRP), a model-based approach,
	            is gaining traction against the traditional weighted approach for
	            survey estimates. MRP estimates are susceptible to bias if there
	            is an underlying structure that the methodology does not capture.
	            This work aims to provide a new framework for specifying
	            structured prior distributions that lead to bias reduction in MRP
	            estimates. We use simulation studies to explore the benefit of
	            these prior distributions and demonstrate their efficacy on
	            non-representative US survey data. We show that structured prior
	            distributions offer absolute bias reduction and variance
	            reduction for posterior MRP estimates, regardless of data regime.
	            },
	archiveprefix = {arxiv},
	langid = {english},
	keywords = {Statistics - Methodology},
	file = {/Users/tedgro/Zotero/storage/953ZS9G6/Gao et al. - 2019 - Improving
	        multilevel regression and poststratifica.pdf},
}


@book{kruschkejohnk.DoingBayesianData2015,
	title = {Doing {{Bayesian Data Analysis}}: A {{Tutorial}} with {{R}}, {{JAGS
	         }}, and {{Stan}}},
	author = {{Kruschke, John K.}},
	year = {2015},
	edition = {2},
	publisher = {Elsevier},
	doi = {10.1016/B978-0-12-405888-0.09999-2},
	url = {
	       https://www.sciencedirect.com/science/article/pii/B9780124058880099992?via%3Dihub#s0010
	       },
	urldate = {2024-09-27},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-12-405888-0},
	langid = {english},
	file = {/Users/tedgro/Zotero/storage/FTVG273L/2015 - Front Matter.pdf},
}

@article{wangPredictingMultidomainProgression2017,
	title = {Predicting the Multi-Domain Progression of {{Parkinson}}'s Disease:
	         A {{Bayesian}} Multivariate Generalized Linear Mixed-Effect Model},
	shorttitle = {Predicting the Multi-Domain Progression of {{Parkinson}}'s
	              Disease},
	author = {Wang, Ming and Li, Zheng and Lee, Eun Young and Lewis, Mechelle M.
	          and Zhang, Lijun and Sterling, Nicholas W. and Wagner, Daymond and
	          Eslinger, Paul and Du, Guangwei and Huang, Xuemei},
	year = {2017},
	month = sep,
	journal = {BMC Medical Research Methodology},
	volume = {17},
	number = {1},
	pages = {147},
	issn = {1471-2288},
	doi = {10.1186/s12874-017-0415-4},
	url = {https://doi.org/10.1186/s12874-017-0415-4},
	urldate = {2024-09-27},
	abstract = {It is challenging for current statistical models to predict
	            clinical progression of Parkinson's disease (PD) because of the
	            involvement of multi-domains and longitudinal data.},
	langid = {english},
	keywords = {Dynamic prediction,Generalized linear mixed-effect model,
	            Imbalance,Motor symptoms,Multivariate longitudinal data,Non-motor
	            symptoms,Parkinson's disease},
	file = {/Users/tedgro/Zotero/storage/5W7XJMUT/Wang et al. - 2017 -
	        Predicting the multi-domain progression of Parkins.pdf},
}

@article{sorensenBayesianLinearMixed2016,
	title = {Bayesian Linear Mixed Models Using {{Stan}}: {{A}} Tutorial for
	         Psychologists, Linguists, and Cognitive Scientists},
	shorttitle = {Bayesian Linear Mixed Models Using {{Stan}}},
	author = {Sorensen, Tanner and Vasishth, Shravan},
	year = {2016},
	month = oct,
	journal = {The Quantitative Methods for Psychology},
	volume = {12},
	number = {3},
	eprint = {1506.06201},
	primaryclass = {stat},
	pages = {175--200},
	issn = {2292-1354},
	doi = {10.20982/tqmp.12.3.p175},
	url = {http://arxiv.org/abs/1506.06201},
	urldate = {2024-09-27},
	abstract = {With the arrival of the R packages nlme and lme4, linear mixed
	            models (LMMs) have come to be widely used in
	            experimentally-driven areas like psychology, linguistics, and
	            cognitive science. This tutorial provides a practical
	            introduction to fitting LMMs in a Bayesian framework using the
	            probabilistic programming language Stan. We choose Stan (rather
	            than WinBUGS or JAGS) because it provides an elegant and scalable
	            framework for fitting models in most of the standard applications
	            of LMMs. We ease the reader into fitting increasingly complex
	            LMMs, first using a two-condition repeated measures self-paced
	            reading study, followed by a more complex \$2{\textbackslash}
	            times 2\$ repeated measures factorial design that can be
	            generalized to much more complex designs.},
	archiveprefix = {arXiv},
	keywords = {Statistics - Methodology},
	file = {/Users/tedgro/Zotero/storage/B75EZEW6/Sorensen and Vasishth - 2016 -
	        Bayesian linear mixed models using Stan A
	        tutoria.pdf;/Users/tedgro/Zotero/storage/RE978SI8/1506.html},
}

@article{cobigoDetectionEmergingNeurodegeneration2022,
	title = {Detection of Emerging Neurodegeneration Using {{Bayesian}} Linear
	         Mixed-Effect Modeling},
	author = {Cobigo, Yann and Goh, Matthew S. and Wolf, Amy and Staffaroni,
	          Adam M. and Kornak, John and Miller, Bruce L. and Rabinovici, Gil
	          D. and Seeley, William W. and Spina, Salvatore and Boxer, Adam L.
	          and Boeve, Bradley F. and Wang, Lei and Allegri, Ricardo and Farlow
	          , Marty and Mori, Hiroshi and Perrin, Richard J. and Kramer, Joel
	          and Rosen, Howard J.},
	year = {2022},
	month = jan,
	journal = {NeuroImage: Clinical},
	volume = {36},
	pages = {103144},
	issn = {2213-1582},
	doi = {10.1016/j.nicl.2022.103144},
	url = {https://www.sciencedirect.com/science/article/pii/S2213158222002091},
	urldate = {2024-09-27},
	abstract = {Early detection of neurodegeneration, and prediction of when
	            neurodegenerative diseases will lead to symptoms, are critical
	            for developing and initiating disease modifying treatments for
	            these disorders. While each neurodegenerative disease has a
	            typical pattern of early changes in the brain, these disorders
	            are heterogeneous, and early manifestations can vary greatly
	            across people. Methods for detecting emerging neurodegeneration
	            in any part of the brain are therefore needed. Prior publications
	            have described the use of Bayesian linear mixed-effects (BLME)
	            modeling for characterizing the trajectory of change across the
	            brain in healthy controls and patients with neurodegenerative
	            disease. Here, we use an extension of such a model to detect
	            emerging neurodegeneration in cognitively healthy individuals at
	            risk for dementia. We use BLME to quantify individualized rates
	            of volume loss across the cerebral cortex from the first two MRIs
	            in each person and then extend the BLME model to predict future
	            values for each voxel. We then compare observed values at
	            subsequent time points with the values that were expected from
	            the initial rates of change and identify voxels that are lower
	            than the expected values, indicating accelerated volume loss and
	            neurodegeneration. We apply the model to longitudinal imaging
	            data from cognitively normal participants in the Alzheimer's
	            Disease Neuroimaging Initiative (ADNI), some of whom subsequently
	            developed dementia, and two cognitively normal cases who
	            developed pathology-proven frontotemporal lobar degeneration
	            (FTLD). These analyses identified regions of accelerated volume
	            loss prior to or accompanying the earliest symptoms, and
	            expanding across the brain over time, in all cases. The changes
	            were detected in regions that are typical for the likely diseases
	            affecting each patient, including medial temporal regions in
	            patients at risk for Alzheimer's disease, and insular, frontal,
	            and/or anterior/inferior temporal regions in patients with likely
	            or proven FTLD. In the cases where detailed histories were
	            available, the first regions identified were consistent with
	            early symptoms. Furthermore, survival analysis in the ADNI cases
	            demonstrated that the rate of spread of accelerated volume loss
	            across the brain was a statistically significant predictor of
	            time to conversion to dementia. This method for detection of
	            neurodegeneration is a potentially promising approach for
	            identifying early changes due to a variety of diseases, without
	            prior assumptions about what regions are most likely to be
	            affected first in an individual.},
	keywords = {Alzheimer's Disease,Bayesian linear mixed-effect,Bayesian
	            prediction,Frontotemporal Lobar Degeneration},
	file = {/Users/tedgro/Zotero/storage/NQ6JHL2Q/Cobigo et al. - 2022 -
	        Detection of emerging neurodegeneration using
	        Baye.pdf;/Users/tedgro/Zotero/storage/PCLFIUA9/S2213158222002091.html
	        },
}

@book{brown2014applied,
	title = {Applied Mixed Models in Medicine},
	author = {Brown, Helen and Prescott, Robin},
	year = {2014},
	publisher = {John Wiley \& Sons},
}
@misc{vonesh2006mixed,
	title = {Mixed Models: Theory and Applications},
	author = {Vonesh, Edward F},
	year = {2006},
	publisher = {Taylor \& Francis},
}

@article{fongBayesianInferenceGeneralized2010,
	title = {Bayesian Inference for Generalized Linear Mixed Models},
	author = {Fong, Youyi and Rue, H{\aa}vard and Wakefield, Jon},
	year = {2010},
	month = jul,
	journal = {Biostatistics (Oxford, England)},
	volume = {11},
	number = {3},
	pages = {397--412},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/kxp053},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2883299/},
	urldate = {2024-09-27},
	abstract = {Generalized linear mixed models (GLMMs) continue to grow in
	            popularity due to their ability to directly acknowledge multiple
	            levels of dependency and model different data types. For small
	            sample sizes especially, likelihood-based inference can be
	            unreliable with variance components being particularly difficult
	            to estimate. A Bayesian approach is appealing but has been
	            hampered by the lack of a fast implementation, and the difficulty
	            in specifying prior distributions with variance components again
	            being particularly problematic. Here, we briefly review previous
	            approaches to computation in Bayesian implementations of GLMMs
	            and illustrate in detail, the use of integrated nested Laplace
	            approximations in this context. We consider a number of examples,
	            carefully specifying prior distributions on meaningful quantities
	            in each case. The examples cover a wide range of data types
	            including those requiring smoothing over time and a relatively
	            complicated spline model for which we examine our prior
	            specification in terms of the implied degrees of freedom. We
	            conclude that Bayesian inference is now practically feasible for
	            GLMMs and provides an attractive alternative to likelihood-based
	            approaches such as penalized quasi-likelihood. As with
	            likelihood-based approaches, great care is required in the
	            analysis of clustered binary data since approximation strategies
	            may be less accurate for such data.},
	pmcid = {PMC2883299},
	pmid = {19966070},
	file = {/Users/tedgro/Zotero/storage/EKNXNZZP/Fong et al. - 2010 - Bayesian
	        inference for generalized linear mixed mo.pdf},
}

@article{gelmanBayesianDataAnalysis2020a,
	title = {Bayesian {{Data Analysis}}, {{Third Edition}}},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson,
	          David B and Vehtari, Aki and Rubin, Donald B},
	year = {2020},
	pages = {656},
	langid = {english},
	file = {/Users/tedgro/Zotero/storage/FIVW4WD9/Gelman et al. - Bayesian Data
	        Analysis, Third Edition.pdf},
}

@article{carpenterStanProbabilisticProgramming2017,
	title = {Stan: {{A Probabilistic Programming Language}}},
	shorttitle = {Stan},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee,
	          Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker,
	          Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017},
	month = jan,
	journal = {Journal of Statistical Software},
	volume = {76},
	number = {1},
	pages = {1--32},
	issn = {1548-7660},
	doi = {10.18637/jss.v076.i01},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v076i01},
	urldate = {2020-08-27},
	copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D.
	             Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus
	             Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
	langid = {english},
	keywords = {algorithmic differentiation,Bayesian inference,probabilistic
	            programming,Stan},
	file = {/Users/tedgro/Zotero/storage/F4VI282K/Carpenter et al. - 2017 - Stan
	        A Probabilistic Programming
	        Language.pdf;/Users/tedgro/Zotero/storage/JNJQGTST/v076i01.html},
}

@article{abril-pla_pymc_2023,
	title = {{PyMC}: a modern, and comprehensive probabilistic programming framework in {Python}},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2376-5992},
	shorttitle = {{PyMC}},
	url = {https://peerj.com/articles/cs-1516},
	doi = {10.7717/peerj-cs.1516},
	abstract = {PyMC is a probabilistic programming library for Python that provides tools for constructing and fitting Bayesian models. It offers an intuitive, readable syntax that is close to the natural syntax statisticians use to describe models. PyMC leverages the symbolic computation library PyTensor, allowing it to be compiled into a variety of computational backends, such as C, JAX, and Numba, which in turn offer access to different computational architectures including CPU, GPU, and TPU. Being a general modeling framework, PyMC supports a variety of models including generalized hierarchical linear regression and classification, time series, ordinary differential equations (ODEs), and non-parametric models such as Gaussian processes (GPs). We demonstrate PyMC‚Äôs versatility and ease of use with examples spanning a range of common statistical models. Additionally, we discuss the positive role of PyMC in the development of the open-source ecosystem for probabilistic programming.},
	language = {en},
	urldate = {2024-11-15},
	journal = {PeerJ Computer Science},
	author = {Abril-Pla, Oriol and Andreani, Virgile and Carroll, Colin and Dong, Larry and Fonnesbeck, Christopher J. and Kochurov, Maxim and Kumar, Ravin and Lao, Junpeng and Luhmann, Christian C. and Martin, Osvaldo A. and Osthege, Michael and Vieira, Ricardo and Wiecki, Thomas and Zinkov, Robert},
	month = sep,
	year = {2023},
	pages = {e1516},
}

@article{capretto_bambi_2022,
	title = {\textbf{{Bambi}} : {A} {Simple} {Interface} for {Fitting} {Bayesian} {Linear} {Models} in \textit{{Python}}},
	volume = {103},
	issn = {1548-7660},
	shorttitle = {\textbf{{Bambi}}},
	url = {https://www.jstatsoft.org/v103/i15/},
	doi = {10.18637/jss.v103.i15},
	language = {en},
	number = {15},
	urldate = {2024-11-15},
	journal = {Journal of Statistical Software},
	author = {Capretto, Tom√°s and Piho, Camen and Kumar, Ravin and Westfall, Jacob and Yarkoni, Tal and Martin, Osvaldo A.},
	year = {2022},
}
@article{psis,
  author  = {Aki Vehtari and Daniel Simpson and Andrew Gelman and Yuling Yao and Jonah Gabry},
  title   = {Pareto Smoothed Importance Sampling},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {72},
  pages   = {1--58},
  url     = {http://jmlr.org/papers/v25/19-556.html}
}
